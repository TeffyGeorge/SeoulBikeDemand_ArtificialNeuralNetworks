# -*- coding: utf-8 -*-
"""Assignment 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wp5tmkZEDdy1rqgnyMDTAuyzId4xW7Ci
"""

#Loading the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
import os 
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import RandomizedSearchCV
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/gdrive')

path = '/content/gdrive/MyDrive/ML/SeoulBikeData.csv'
bike_df = pd.read_csv(path, encoding='cp1252')
print("Info:-")
bike_df.info()
print("Describe:-")
bike_df.describe()

#checking missing values
bike_df.isna().sum()
bike_df.isnull().sum()

#checking duplicate values
print('Duplicate values : ', len(bike_df[bike_df.duplicated()]))

# Dependent variable is Rented Bike Count. 
# We will change this numerical output to a binary classification 
# Adding a new variable with threshold set 
threshold = bike_df['Rented Bike Count'].quantile(0.75)
threshold

# We set the threshold to 1065.25, which is the 75th percentile for Rented Bike Count.
# Every field with a bike count above 1065.25 will be labeled 1, 
# and rest will be labeled 0.
# threshold = 1065.25
bike_df["Rented_Bike_Count_Value"] = (bike_df['Rented Bike Count'] > threshold).astype(float)
bike_df.head(5000)

"""DATA PREPROCESSING"""

# Add few variables (split the date to month, year,weekday and day)
bike_df['Date']=pd.to_datetime(bike_df['Date'])
from datetime import datetime
import datetime as dt

bike_df['Year']=bike_df['Date'].dt.year
bike_df['Month']=bike_df['Date'].dt.month
bike_df['Day']=bike_df['Date'].dt.day
bike_df['DayName']=bike_df['Date'].dt.day_name()
bike_df['Weekday'] = bike_df['DayName'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )
bike_df=bike_df.drop(columns=['Date','DayName','Year'],axis=1)

#Holiday, Functioning Day and Seasons need to have dummy variables  (object DType)
bike_df = pd.get_dummies(bike_df, columns = ['Seasons',	'Holiday',	'Functioning Day'])

bike_df.head()

"""DECLARE FEATURE VECTOR AND TARGET VARIABLE"""

# Split the features in X and Y
X = bike_df.drop(columns=['Rented_Bike_Count_Value','Rented Bike Count'], axis=1)
y = bike_df['Rented_Bike_Count_Value']

"""SPLIT DATA TO TRAINING AND TEST SET"""

#Create test and train data
from sklearn.model_selection import train_test_split
#split the data by percentage
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)

#checking the shape of X_train and X_test
print(X_train.shape)
print(X_test.shape)

"""FEATURE SCALING"""

cols = X_train.columns

print("------Data Standardization------")
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

X_train = pd.DataFrame(X_train, columns=[cols])

X_test = pd.DataFrame(X_test, columns=[cols])

X_train.describe()

"""ANN - Artificial Neural Network"""

import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()

# Adding the input layer and the first hidden layer
model.add(Dense(10, activation = 'relu', input_dim = 20))

# Adding the second hidden layer
model.add(Dense(10, activation = 'relu'))

# Adding the output layer
model.add(Dense(1,  activation = 'sigmoid'))

# Compiling the ANN
model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Fitting the ANN to the Training set
model.fit(X_train, y_train.values, batch_size = 10, epochs = 10)

from sklearn.metrics import confusion_matrix
def accuracy_NN(model,X,y):
    # Predicting the Test set results
    y_pred = model.predict(X)
    y_pred = (y_pred > 0.5)
    cm = confusion_matrix(y.values, y_pred)
    return (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])

print('Train accuracy = {0}%'.format(np.round(accuracy_NN(model,X_train, y_train) * 100, 2)))
print('Test accuracy = {0}%'.format(np.round(accuracy_NN(model,X_test, y_test) * 100, 2)))

from sklearn.metrics import accuracy_score
def accuracyscore(model,X,y):
    # Predicting the Test set results
    y_pred = model.predict(X)
    y_pred = (y_pred > 0.5)
    return accuracy_score(y,y_pred)

print('Train accuracy = {0}%'.format(np.round(accuracyscore(model,X_train, y_train) * 100, 2)))
print('Test accuracy = {0}%'.format(np.round(accuracyscore(model,X_test, y_test) * 100, 2)))

# Fitting the ANN to the Training set
history = model.fit(X_train, y_train.values, batch_size = 20, epochs = 100,validation_split = 0.2)

import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

"""1. NO OF HIDDEN LAYERS """

accuracy_validation = [None]*10
for hiddenLayersCount in range(0,10):
    
    model = Sequential()
    # First Layer
    model.add(Dense( 10, activation = 'relu', input_dim = 20))
    i=0
    # N Number of Hidden Layers
    while(i < hiddenLayersCount):
        model.add(Dense( 10,  activation = 'relu'))
        i+=1
    
    # Final Layer
    model.add(Dense( 1,  activation = 'sigmoid'))
    
    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
    model.fit(X_train, y_train.values, batch_size = 10, epochs = 4)
    accuracy_validation[hiddenLayersCount]=np.round(accuracy_NN(model,X_test, y_test) * 100, 2)

plt.ylim([0,100])
plt.plot(accuracy_validation)
plt.title("Accuracy vs No of Hidden Layers")
plt.xlabel("No of Hidden Layers")
plt.ylabel("Accuracy on Validation Set")

"""Number of Hidden layers won't increase significant accuracy,
We will keep only 1 hidden layer in the NN

2. No of Nodes in Hidden Layer
"""

accuracy_validation = [None]*10
count = 0
for noOfNodes in range(10,20):
    
    classifier = Sequential()
    # First Layer + Hidden Layer
    classifier.add(Dense(noOfNodes, activation = 'relu', input_dim = 20))
    
    # Second Hidden Layer
    classifier.add(Dense( noOfNodes, activation = 'relu'))

    # Final Layer
    classifier.add(Dense(1, activation = 'sigmoid'))
    
    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
    classifier.fit(X_train, y_train.values, batch_size = 10, epochs = 4)
    accuracy_validation[count]=np.round(accuracy_NN(classifier,X_test, y_test) * 100, 2)
    count+=1

plt.ylim([0,100])
plt.plot(range(20,30),accuracy_validation)
plt.title("Accuracy vs No of nodes in hidden Layers")
plt.xlabel("No of nodes in Hidden Layers")
plt.ylabel("Accuracy on Validation Set")

"""No of layers according to our data set doesn't really change the accuracy
We can go ahead with 10 number of Nodes

3. Change Activation Function
"""

accuracy_validation = [None]*3
count = 0
activationFunc = ['relu','sigmoid','tanh']
for activation_Func in range(len(activationFunc)):
    
    classifier = Sequential()

    classifier.add(Dense(10, activation = 'relu', input_dim = 20))
    
    classifier.add(Dense(10,activation = 'relu'))

    classifier.add(Dense(1, activation = activationFunc[activation_Func]))
    
    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
    classifier.fit(X_train, y_train.values, batch_size = 10, epochs = 4)
    accuracy_validation[count]=np.round(accuracy_NN(classifier,X_test, y_test) * 100, 2)
    count+=1

plt.ylim([0,100])
plt.plot(activationFunc,accuracy_validation)
plt.title("Accuracy vs Activation Function")
plt.xlabel("Activation Functions")
plt.ylabel("Accuracy on Validation Set")

"""We will use sigmoid activation function

4. Batch size for error back propagation
"""

accuracy_validation = [None]*10
count = 0
for batchSize in range(5,15):
    
    classifier = Sequential()
    classifier.add(Dense(10, activation = 'relu', input_dim = 20))
    classifier.add(Dense(10, activation = 'relu'))

    
    # Final Layer
    classifier.add(Dense(1, activation = 'relu'))
    
    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
    classifier.fit(X_train, y_train.values, batch_size = batchSize, epochs = 3)
    accuracy_validation[count]=np.round(accuracy_NN(classifier,X_test, y_test) * 100, 2)
    count+=1

plt.ylim([0,100])
plt.plot(range(5,15),accuracy_validation)
plt.title("Accuracy vs Batch Size")
plt.xlabel("Batch Size")
plt.ylabel("Accuracy on Validation Set")

"""Batch Size is good for 8

FINAL ANN
"""

# Adding the input layer and the first hidden layer
classifier.add(Dense(10, activation = 'relu', input_dim = 20))

# Adding the second hidden layer
classifier.add(Dense(10, activation = 'relu'))

# Adding the output layer
classifier.add(Dense(1, activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Fitting the ANN to the Training set
classifier.fit(X_train, y_train.values, batch_size = 8, epochs = 20)

print('Train accuracy = {0}%'.format(np.round(accuracy_NN(classifier,X_train, y_train) * 100, 2)))
print('Test accuracy = {0}%'.format(np.round(accuracy_NN(classifier,X_test, y_test) * 100, 2)))

"""USING GRIDSEARCHCV ON THE MODELS

1. Hyperparameters: Batch size and # of Epochs
"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, plot_roc_curve
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasClassifier
#from keras.optimizers import SGD
from keras.constraints import maxnorm
from warnings import filterwarnings

c_train = c[X.index.isin(X_train.index)]
c_test = c[~X.index.isin(X_train.index)]
bike_df["Rented_Bike_Count_Value"]

def model_1():
    model = Sequential()
    #model.add(Input(shape = (40, )))
    model.add(Dense(10, activation = 'sigmoid'))
    model.add(Dense(1, activation = 'sigmoid'))
    model.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])
    return model

nnc1_model = KerasClassifier(build_fn = model_1, verbose = 0)

nnc1_batch_size_arr = [10, 20, 50]
nnc1_epoch_arr = [10, 50, 100]

nnc1 = GridSearchCV(estimator = nnc1_model, 
                    param_grid = {'batch_size' : nnc1_batch_size_arr, 
                                  'epochs' : nnc1_epoch_arr}, 
                    n_jobs = -1, cv = 5, return_train_score = True)

nnc1.fit(X_train.values, y_train.values.ravel())

nnc1_df = pd.DataFrame(nnc1.cv_results_)

import seaborn as sns
print('Best Parameters: {}'.format(nnc1.best_params_))
print('Best Cross-Validation Accuracy: {:.2f}'.format(nnc1.best_score_))

pred_nnc1 = nnc1.predict(X_test)

# plot_roc_curve(nnc1, X_test, c_test)
# plt.title('ROC Curve')
# plt.show()

cm_nnc1 = confusion_matrix(y_test, pred_nnc1)
tn, fp, fn, tp = cm_nnc1.ravel()

ax = sns.heatmap(cm_nnc1.T, square = True, annot = True, fmt = 'd', cbar = False, cmap = 'Greens')
ax.set(xlabel = 'Actual', ylabel = 'Predicted', title = 'Confusion Matrix')
plt.show()

print('Accuracy: {:.2f}'.format(accuracy_score(y_test, pred_nnc1)))
print('Sensitivity: {:.2f}'.format(tp / (tp + fn)))
print('Specificity: {:.2f}'.format(tn / (tn + fp)))

"""2. Activation Function of the 1st hidden layer nodes"""

def model_2(activation = 'sigmoid'):
    model = Sequential()
    model.add(Dense(10, activation = activation))
    model.add(Dense(1, activation = 'sigmoid'))
    model.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])
    return model

nnc2_model = KerasClassifier(build_fn = model_2, batch_size = 10, epochs = 100, verbose = 0)

nnc2_activation_arr = ['linear', 'relu', 'sigmoid', 'tanh']

nnc2 = GridSearchCV(estimator = nnc2_model, 
                    param_grid = {'activation' : nnc2_activation_arr}, 
                    n_jobs = -1, cv = 5, return_train_score = True)

nnc2.fit(X_train.values, y_train.values.ravel())

nnc2_df = pd.DataFrame(nnc2.cv_results_)

nnc2_df[['param_activation', 'mean_train_score', 'mean_test_score']]

print('Best Parameters: {}'.format(nnc2.best_params_))
print('Best Cross-Validation Accuracy: {:.2f}'.format(nnc2.best_score_))

pred_nnc2 = nnc2.predict(X_test)

# plot_roc_curve(nnc2, X_test, c_test)
# plt.title('ROC Curve')
# plt.show()

cm_nnc2 = confusion_matrix(y_test, pred_nnc2)
tn, fp, fn, tp = cm_nnc2.ravel()

ax = sns.heatmap(cm_nnc2.T, square = True, annot = True, fmt = 'd', cbar = False, cmap = 'Greens')
ax.set(xlabel = 'Actual', ylabel = 'Predicted', title = 'Confusion Matrix')
plt.show()

print('Accuracy: {:.2f}'.format(accuracy_score(y_test, pred_nnc2)))
print('Sensitivity: {:.2f}'.format(tp / (tp + fn)))
print('Specificity: {:.2f}'.format(tn / (tn + fp)))

"""3.Learning Rate and Momentum of the 1st hidden layer nodes"""

def model_3(learn_rate = 0.01, momentum = 0.0):
    model = Sequential()
    model.add(Dense(10, activation = 'relu'))
    model.add(Dense(1, activation = 'sigmoid'))
    optimizer = keras.optimizers.SGD(lr = learn_rate, momentum = momentum)
    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])
    return model

nnc3_model = KerasClassifier(build_fn = model_3, batch_size = 10, epochs = 100, verbose = 0)

nnc3_learn_rate_arr = [0.001, 0.01, 0.1, 0.2, 0.5]
nnc3_momentum_arr = [0.0, 0.1, 0.2, 0.5, 0.9]
 
nnc3 = GridSearchCV(estimator = nnc3_model, 
                    param_grid = {'learn_rate' : nnc3_learn_rate_arr,
                                  'momentum' : nnc3_momentum_arr}, 
                    n_jobs = -1, cv = 5, return_train_score = True)

nnc3.fit(X_train.values, y_train.values.ravel())

nnc3_df = pd.DataFrame(nnc3.cv_results_)

jnnc3_train_err_mat = (1 - nnc3_df['mean_train_score']).values.reshape(len(nnc3_learn_rate_arr), len(nnc3_momentum_arr))
ax = sns.heatmap(pd.DataFrame(nnc3_train_err_mat, columns = nnc3_momentum_arr, index = nnc3_learn_rate_arr).iloc[::-1], 
                 annot = True, cmap = 'Greens_r')
ax.set(xlabel = 'Momentum', ylabel = 'Learning Rate', title = 'Train Error Rate vs Tuning Parameters')
plt.show()

nnc3_cv_err_mat = (1 - nnc3_df['mean_test_score']).values.reshape(len(nnc3_learn_rate_arr), len(nnc3_momentum_arr))
ax = sns.heatmap(pd.DataFrame(nnc3_cv_err_mat, columns = nnc3_momentum_arr, index = nnc3_learn_rate_arr).iloc[::-1], 
                 annot = True, cmap = 'Greens_r')
ax.set(xlabel = 'Momentum', ylabel = 'Learning Rate', title = 'CV Error Rate vs Tuning Parameters')
plt.show()

print('Best Parameters: {}'.format(nnc3.best_params_))
print('Best Cross-Validation Accuracy: {:.2f}'.format(nnc3.best_score_))

pred_nnc3 = nnc3.predict(X_test)

# plot_roc_curve(nnc3, X_test, c_test)
# plt.title('ROC Curve')
# plt.show()

cm_nnc3 = confusion_matrix(y_test, pred_nnc3)
tn, fp, fn, tp = cm_nnc3.ravel()

ax = sns.heatmap(cm_nnc3.T, square = True, annot = True, fmt = 'd', cbar = False, cmap = 'Greens')
ax.set(xlabel = 'Actual', ylabel = 'Predicted', title = 'Confusion Matrix')
plt.show()

print('Accuracy: {:.2f}'.format(accuracy_score(c_test, pred_nnc3)))
print('Sensitivity: {:.2f}'.format(tp / (tp + fn)))
print('Specificity: {:.2f}'.format(tn / (tn + fp)))

"""4.Drop-out Rate and Weight Constraint of the 1st hidden layer nodes"""

def model_4(dropout_rate = 0.0, weight_constraint = 0):
    model = Sequential()
    model.add(Dense(10, activation = 'relu', kernel_constraint = maxnorm(weight_constraint)))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation = 'sigmoid'))
    optimizer = keras.optimizers.SGD(lr = 0.01, momentum = 0.9)
    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics= ['accuracy'])
    return model

nnc4_model = KerasClassifier(build_fn = model_4, batch_size = 10, epochs = 100, verbose = 0)

nnc4_wgt_constraint_arr = [0, 1, 2, 3, 4, 5]
nnc4_dropout_rate_arr = [0.0, 0.1, 0.2, 0.5, 0.9]

nnc4 = GridSearchCV(estimator = nnc4_model, 
                    param_grid = {'weight_constraint' : nnc4_wgt_constraint_arr,
                                  'dropout_rate' : nnc4_dropout_rate_arr}, 
                    n_jobs = -1, cv = 5, return_train_score = True)

nnc4.fit(X_train.values, y_train.values.ravel())

nnc4_df = pd.DataFrame(nnc4.cv_results_)

nnc4_train_err_mat = (1 - nnc4_df['mean_train_score']).values.reshape(len(nnc4_wgt_constraint_arr), 
                                                                      len(nnc4_dropout_rate_arr))
ax = sns.heatmap(pd.DataFrame(nnc4_train_err_mat, columns = nnc4_dropout_rate_arr, 
                              index = nnc4_wgt_constraint_arr).iloc[::-1], annot = True, cmap = 'Greens_r')
ax.set(xlabel = 'Drop-out Rate', ylabel = 'Max Norm Weight', title = 'Train Error Rate vs Tuning Parameters')
plt.show()

nnc4_cv_err_mat = (1 - nnc4_df['mean_test_score']).values.reshape(len(nnc4_wgt_constraint_arr), len(nnc4_dropout_rate_arr))
ax = sns.heatmap(pd.DataFrame(nnc4_cv_err_mat, columns = nnc4_dropout_rate_arr, 
                              index = nnc4_wgt_constraint_arr).iloc[::-1], annot = True, cmap = 'Greens_r')
ax.set(xlabel = 'Drop-out Rate', ylabel = 'Max Norm Weight', title = 'CV Error Rate vs Tuning Parameters')
plt.show()

print('Best Parameters: {}'.format(nnc4.best_params_))
print('Best Cross-Validation Accuracy: {:.2f}'.format(nnc4.best_score_))

pred_nnc4 = nnc4.predict(X_test)

# plot_roc_curve(nnc4, X_test, c_test)
# plt.title('ROC Curve')
# plt.show()

cm_nnc4 = confusion_matrix(y_test, pred_nnc4)
tn, fp, fn, tp = cm_nnc4.ravel()

ax = sns.heatmap(cm_nnc4.T, square = True, annot = True, fmt = 'd', cbar = False, cmap = 'Greens')
ax.set(xlabel = 'Actual', ylabel = 'Predicted', title = 'Confusion Matrix')
plt.show()

print('Accuracy: {:.2f}'.format(accuracy_score(y_test, pred_nnc4)))
print('Sensitivity: {:.2f}'.format(tp / (tp + fn)))
print('Specificity: {:.2f}'.format(tn / (tn + fp)))

"""5. NO Of Neurons in First Hidden Layer"""

def model_5(neurons = 0):
    model = Sequential()
    model.add(Dense(neurons, activation = 'relu', kernel_constraint = maxnorm(3)))
    model.add(Dropout(0.1))
    model.add(Dense(1, activation = 'sigmoid'))
    optimizer = keras.optimizers.SGD(lr = 0.01, momentum = 0.9)
    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics= ['accuracy'])
    return model

nnc5_model = KerasClassifier(build_fn = model_5, batch_size = 10, epochs = 100, verbose = 0)

nnc5_neuron_arr = [1, 5, 10, 15, 20, 25, 30]

nnc5 = GridSearchCV(estimator = nnc5_model, 
                    param_grid = {'neurons' : nnc5_neuron_arr}, 
                    n_jobs = -1, cv = 5, return_train_score = True)

nnc5.fit(X_train.values, y_train.values.ravel())

nnc5_df = pd.DataFrame(nnc5.cv_results_)

nnc5_df[['param_neurons', 'mean_train_score', 'mean_test_score']]

print('Best Parameters: {}'.format(nnc5.best_params_))
print('Best Cross-Validation Accuracy: {:.2f}'.format(nnc5.best_score_))

pred_nnc5 = nnc5.predict(X_test)

# plot_roc_curve(nnc5, X_test, c_test)
# plt.title('ROC Curve')
# plt.show()

cm_nnc5 = confusion_matrix(y_test, pred_nnc5)
tn, fp, fn, tp = cm_nnc5.ravel()

ax = sns.heatmap(cm_nnc5.T, square = True, annot = True, fmt = 'd', cbar = False, cmap = 'Greens')
ax.set(xlabel = 'Actual', ylabel = 'Predicted', title = 'Confusion Matrix')
plt.show()

print('Accuracy: {:.2f}'.format(accuracy_score(y_test, pred_nnc5)))
print('Sensitivity: {:.2f}'.format(tp / (tp + fn)))
print('Specificity: {:.2f}'.format(tn / (tn + fp)))

"""6. No Of Layers in Second Hidden Layer"""

def model_6(neurons = 0):
    model = Sequential()
    model.add(Dense(10, activation = 'relu', kernel_constraint = maxnorm(3)))
    model.add(Dropout(0.1))
    model.add(Dense(neurons, activation = 'relu', kernel_constraint = maxnorm(3)))
    model.add(Dropout(0.1))
    model.add(Dense(1, activation = 'sigmoid'))
    optimizer = keras.optimizers.SGD(lr = 0.01, momentum = 0.9)
    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics= ['accuracy'])
    return model

nnc6_model = KerasClassifier(build_fn = model_6, batch_size = 10, epochs = 100, verbose = 0)

nnc6_neuron_arr = [1, 5, 10, 15, 20, 25, 30]

nnc6 = GridSearchCV(estimator = nnc6_model, 
                    param_grid = {'neurons' : nnc6_neuron_arr}, 
                    n_jobs = -1, cv = 5, return_train_score = True)

nnc6.fit(X_train.values, y_train.values.ravel())

nnc6_df = pd.DataFrame(nnc6.cv_results_)

nnc6_df[['param_neurons', 'mean_train_score', 'mean_test_score']]

print('Best Parameters: {}'.format(nnc6.best_params_))
print('Best Cross-Validation Accuracy: {:.2f}'.format(nnc6.best_score_))

pred_nnc6 = nnc6.predict(X_test)

# plot_roc_curve(nnc6, X_test, c_test)
# plt.title('ROC Curve')
# plt.show()

cm_nnc6 = confusion_matrix(y_test, pred_nnc6)
tn, fp, fn, tp = cm_nnc6.ravel()

ax = sns.heatmap(cm_nnc6.T, square = True, annot = True, fmt = 'd', cbar = False, cmap = 'Greens')
ax.set(xlabel = 'Actual', ylabel = 'Predicted', title = 'Confusion Matrix')
plt.show()

print('Accuracy: {:.2f}'.format(accuracy_score(y_test, pred_nnc6)))
print('Sensitivity: {:.2f}'.format(tp / (tp + fn)))
print('Specificity: {:.2f}'.format(tn / (tn + fp)))

# Train and evaluate models for all combinations of parameters specified in the init method. We would like to obtain following outputs:
  #   1. Training Accuracy and Error (Loss) for every model
  #   2. Test Accuracy and Error (Loss) for every model
  #   3. History Curve (Plot of Accuracy against training steps) for all the models in a single plot. The plot should be color coded i.e.different color for each model
def train_evaluate(actFunc,hidden_layer_size,epoch,lrate):
      #X_train, X_test, y_train, y_test = train_test_split(self.processed_data, self.y, test_size=0.2, shuffle = True, random_state =1)
      
      model = keras.Sequential()
      model.add(Dense(16, input_dim =20, kernel_initializer= 'normal' , activation= actFunc))
      if hidden_layer_size==3:
        model.add(Dense(10, kernel_initializer= 'normal' , activation= actFunc ))
      model.add(Dense(1, kernel_initializer= 'normal' , activation= actFunc ))
      
      # Compile model
      opt = keras.optimizers.Adam(learning_rate=lrate)
      model.compile(loss= 'mean_squared_error' , optimizer= opt , metrics=[ 'accuracy' ])
      history = model.fit(X_train, y_train, epochs= epoch, validation_data= (X_test,y_test), batch_size=20, verbose=0)
      y_pred = model.predict(X_test)
      pred_train= model.predict(X_train)
      train_scores = model.evaluate(X_train, y_train, verbose=0)
      print('Training Accuracy : {} \nTraining Error : {}'.format(round(train_scores[1],4), round(train_scores[0],4)))   
      
      pred_test= model.predict(X_test)
      test_scores = model.evaluate(X_test, y_test, verbose=0)
      print('Testing Accuracy : {} \nTesting Error : {}'.format(round(test_scores[1],4), round(test_scores[0],4))) 
    
      return history

def plot_training_graph( sigmoidHistory, tanhHistory, reluHistory, epoch, lrate, layers, ax):   
      ax.plot(sigmoidHistory.history['accuracy'], color='green', linestyle='solid')
      ax.plot(tanhHistory.history['accuracy'], color='red', linestyle='solid')
      ax.plot(reluHistory.history['accuracy'], color='blue', linestyle='solid')
      ax.plot(sigmoidHistory.history['loss'], color='green', linestyle='dashed')
      ax.plot(tanhHistory.history['loss'], color='red', linestyle='dashed')
      ax.plot(reluHistory.history['loss'], color='blue', linestyle='dashed')
      ax.set_title('Epoch='+str(Epoch)+', Learning Rate='+str(LearningRate)+', Hidden Layers='+str(Layers))       

def plot_testing_graph(sigmoidHistory, tanhHistory, reluHistory, epoch, lrate, layers, ax):
      ax.plot(sigmoidHistory.history['val_accuracy'], color='green', linestyle='solid')
      ax.plot(tanhHistory.history['val_accuracy'], color='red', linestyle='solid')
      ax.plot(reluHistory.history['val_accuracy'], color='blue', linestyle='solid')
      ax.plot(sigmoidHistory.history['val_loss'], color='green', linestyle='dashed')
      ax.plot(tanhHistory.history['val_loss'], color='red', linestyle='dashed')
      ax.plot(reluHistory.history['val_loss'], color='blue', linestyle='dashed')
      ax.set_title('Epoch='+str(Epoch)+', Learning Rate='+str(LearningRate)+', Hidden Layers='+str(Layers))

# List of hyperparameters to be used for for model evaluation
hpList = [{'epoch' : 100, 'learning_rate' : 0.001, 'layers' : 2},
              {'epoch' : 100, 'learning_rate' : 0.001, 'layers' : 3},
              {'epoch' : 100, 'learning_rate' : 0.01, 'layers' : 2},
              {'epoch' : 100, 'learning_rate' : 0.01, 'layers' : 3},
              {'epoch' : 200, 'learning_rate' : 0.001, 'layers' : 2},
              {'epoch' : 200, 'learning_rate' : 0.001, 'layers' : 3},
              {'epoch' : 200, 'learning_rate' : 0.01, 'layers' : 2},
              {'epoch' : 200, 'learning_rate' : 0.01, 'layers' : 3},
             ]
            
count=0;
labels = ["Sigmoid Accuracy", "TanH Accuracy","Relu Accuracy","Sigmoid Loss","TanH Loss","Relu Loss"]

fig1 = plt.figure(edgecolor='black', linewidth=4)
fig2 = plt.figure(edgecolor='black', linewidth=4)

for dic in hpList:
        count=count+1
        print('\n------------------------- Case -',count, '-------------------------')
        Layers = dic.get('layers')
        Epoch = dic.get('epoch')
        LearningRate = dic.get('learning_rate')
        print(f'Hidden Layers={Layers}, Epoch={Epoch}, Learning Rate={LearningRate}')
        print('\n-----------Sigmoid-----------')
        sigmoidHistory = train_evaluate('sigmoid',Layers,Epoch,LearningRate)

        print('\n-----------Tanh-----------')
        tanhHistory = train_evaluate('tanh',Layers,Epoch,LearningRate)

        print('\n-----------Relu-----------')
        reluHistory = train_evaluate('relu',Layers,Epoch,LearningRate)

        ax1 = fig1.add_subplot(4,2,count)
        ax2 = fig2.add_subplot(4,2,count)
        plot_training_graph(sigmoidHistory, tanhHistory, reluHistory, Epoch, LearningRate, Layers, ax1)
        plot_testing_graph(sigmoidHistory, tanhHistory, reluHistory, Epoch, LearningRate, Layers, ax2)

#Plot the model history for each model in a single plot    
fig1.legend(ax1.get_lines(), labels, ncol=2, loc="upper center")
fig2.legend(ax2.get_lines(), labels, ncol=2, loc="upper center")

fig1.set_size_inches(18.5, 25, forward=True)
fig2.set_size_inches(18.5, 25, forward=True)

fig1.text(0.5, 0.09, 'Epoch', ha='center')
fig1.text(0.07, 0.5, 'Training Accuracy/Loss', va='center', rotation='vertical')

fig2.text(0.5, 0.09, 'Epoch', ha='center')
fig2.text(0.07, 0.5, 'Testing Accuracy/Loss', va='center', rotation='vertical')

plt.show()

